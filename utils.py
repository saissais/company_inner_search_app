"""
ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ã‚’å®šç¾©ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã§ã™ã€‚
"""

############################################################
# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®èª­ã¿è¾¼ã¿
############################################################
import os
from dotenv import load_dotenv
import streamlit as st
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema import HumanMessage
from langchain.docstore.document import Document
from langchain_openai import ChatOpenAI
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
import constants as ct

# pandasã¯æ¡ä»¶ä»˜ãã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False

############################################################
# è¨­å®šé–¢é€£
############################################################
# ã€Œ.envã€ãƒ•ã‚¡ã‚¤ãƒ«ã§å®šç¾©ã—ãŸç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()


############################################################
# é–¢æ•°å®šç¾©
############################################################

def get_source_icon(source):
    """
    ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ä¸€ç·’ã«è¡¨ç¤ºã™ã‚‹ã‚¢ã‚¤ã‚³ãƒ³ã®ç¨®é¡ã‚’å–å¾—

    Args:
        source: å‚ç…§å…ƒã®ã‚ã‚Šã‹

    Returns:
        ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ä¸€ç·’ã«è¡¨ç¤ºã™ã‚‹ã‚¢ã‚¤ã‚³ãƒ³ã®ç¨®é¡
    """
    # å‚ç…§å…ƒãŒWebãƒšãƒ¼ã‚¸ã®å ´åˆã¨ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã§ã€å–å¾—ã™ã‚‹ã‚¢ã‚¤ã‚³ãƒ³ã®ç¨®é¡ã‚’å¤‰ãˆã‚‹
    if source.startswith("http"):
        icon = ct.LINK_SOURCE_ICON
    else:
        icon = ct.DOC_SOURCE_ICON
    
    return icon


def build_error_message(message):
    """
    ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ç®¡ç†è€…å•ã„åˆã‚ã›ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®é€£çµ

    Args:
        message: ç”»é¢ä¸Šã«è¡¨ç¤ºã™ã‚‹ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

    Returns:
        ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ç®¡ç†è€…å•ã„åˆã‚ã›ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®é€£çµãƒ†ã‚­ã‚¹ãƒˆ
    """
    return "\n".join([message, ct.COMMON_ERROR_MESSAGE])


def get_llm_response(chat_message):
    """
    LLMã‹ã‚‰ã®å›ç­”å–å¾—

    Args:
        chat_message: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å€¤

    Returns:
        LLMã‹ã‚‰ã®å›ç­”ï¼ˆcontextæƒ…å ±ã‚‚å«ã‚€ï¼‰
    """
    # LLMã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç”¨æ„
    llm = ChatOpenAI(model_name=ct.MODEL, temperature=ct.TEMPERATURE)

    # ä¼šè©±å±¥æ­´ãªã—ã§ã‚‚LLMã«ç†è§£ã—ã¦ã‚‚ã‚‰ãˆã‚‹ã€ç‹¬ç«‹ã—ãŸå…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆ
    question_generator_template = ct.SYSTEM_PROMPT_CREATE_INDEPENDENT_TEXT
    question_generator_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", question_generator_template),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}")
        ]
    )

    # ãƒ¢ãƒ¼ãƒ‰ã«ã‚ˆã£ã¦LLMã‹ã‚‰å›ç­”ã‚’å–å¾—ã™ã‚‹ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å¤‰æ›´
    if st.session_state.mode == ct.ANSWER_MODE_1:
        # ãƒ¢ãƒ¼ãƒ‰ãŒã€Œç¤¾å†…æ–‡æ›¸æ¤œç´¢ã€ã®å ´åˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        question_answer_template = ct.SYSTEM_PROMPT_DOC_SEARCH
    else:
        # ãƒ¢ãƒ¼ãƒ‰ãŒã€Œç¤¾å†…å•ã„åˆã‚ã›ã€ã®å ´åˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        question_answer_template = ct.SYSTEM_PROMPT_INQUIRY
    # LLMã‹ã‚‰å›ç­”ã‚’å–å¾—ã™ã‚‹ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆ
    question_answer_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", question_answer_template),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}")
        ]
    )

    # ä¼šè©±å±¥æ­´ãªã—ã§ã‚‚LLMã«ç†è§£ã—ã¦ã‚‚ã‚‰ãˆã‚‹ã€ç‹¬ç«‹ã—ãŸå…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹ãŸã‚ã®Retrieverã‚’ä½œæˆ
    history_aware_retriever = create_history_aware_retriever(
        llm, st.session_state.retriever, question_generator_prompt
    )

    # LLMã‹ã‚‰å›ç­”ã‚’å–å¾—ã™ã‚‹ç”¨ã®Chainã‚’ä½œæˆ
    question_answer_chain = create_stuff_documents_chain(llm, question_answer_prompt)
    # ã€ŒRAG x ä¼šè©±å±¥æ­´ã®è¨˜æ†¶æ©Ÿèƒ½ã€ã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã®Chainã‚’ä½œæˆ
    chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

    # LLMã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹å–å¾—
    llm_response = chain.invoke({"input": chat_message, "chat_history": st.session_state.chat_history})
    # LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¼šè©±å±¥æ­´ã«è¿½åŠ 
    st.session_state.chat_history.extend([HumanMessage(content=chat_message), llm_response["answer"]])

    return llm_response


def split_documents_with_metadata(documents):
    """
    ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’åˆ†å‰²ã—ã€ãƒšãƒ¼ã‚¸æƒ…å ±ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«ä¿æŒã™ã‚‹
    
    Args:
        documents: åˆ†å‰²å¯¾è±¡ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
        
    Returns:
        åˆ†å‰²ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆï¼ˆãƒšãƒ¼ã‚¸æƒ…å ±ä»˜ãï¼‰
    """
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=ct.CHUNK_SIZE,
        chunk_overlap=ct.CHUNK_OVERLAP
    )
    
    splits = []
    for doc in documents:
        # ãƒšãƒ¼ã‚¸æƒ…å ±ãŒãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
        if 'page' in doc.metadata:
            page_num = doc.metadata['page']
        else:
            page_num = 0  # PyMuPDFã¯0ãƒ™ãƒ¼ã‚¹ãªã®ã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¯0
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’åˆ†å‰²
        doc_splits = text_splitter.split_documents([doc])
        
        # å„åˆ†å‰²ã«ãƒšãƒ¼ã‚¸æƒ…å ±ã‚’ä»˜ä¸
        for split in doc_splits:
            split.metadata['page'] = page_num
            splits.append(split)
    
    return splits


def display_answer_with_page_info(response, retrieved_docs):
    """
    å›ç­”ã¨ã‚½ãƒ¼ã‚¹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆãƒšãƒ¼ã‚¸æƒ…å ±ä»˜ãï¼‰ã‚’è¡¨ç¤º
    
    Args:
        response: LLMã‹ã‚‰ã®å›ç­”ãƒ†ã‚­ã‚¹ãƒˆ
        retrieved_docs: æ¤œç´¢ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
    """
    # å›ç­”ã‚’è¡¨ç¤º
    st.write(response)
    
    # ã‚½ãƒ¼ã‚¹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¡¨ç¤º
    if retrieved_docs:
        st.markdown("### å‚è€ƒæ–‡æ›¸")
        
        for i, doc in enumerate(retrieved_docs, 1):
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—
            source = doc.metadata.get('source', 'Unknown')
            filename = source.split('/')[-1] if '/' in source else source
            
            # ãƒšãƒ¼ã‚¸æƒ…å ±ã‚’å–å¾—
            page = doc.metadata.get('page', None)
            
            # ãƒšãƒ¼ã‚¸æƒ…å ±ã‚’å«ã‚ã¦è¡¨ç¤º
            if filename.endswith('.pdf') and page is not None:
                st.markdown(f"**{filename}** (ãƒšãƒ¼ã‚¸ {page + 1})")  # ãƒšãƒ¼ã‚¸ã¯0ãƒ™ãƒ¼ã‚¹ãªã®ã§+1
            else:
                st.markdown(f"**{filename}**")
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å†…å®¹ã‚’è¡¨ç¤º
            st.text(doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content)
            st.markdown("---")


def custom_csv_loader(file_path):
    """
    CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ­£ã—ã„éƒ¨ç½²åˆ—ã§èª­ã¿è¾¼ã‚€ï¼ˆä¿®æ­£ç‰ˆï¼‰
    
    Args:
        file_path: CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        
    Returns:
        æ­£ã—ãäººäº‹éƒ¨ã‚’ç‰¹å®šã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
    """
    # pandasãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯å¾“æ¥ã®æ–¹æ³•ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    if not PANDAS_AVAILABLE:
        from langchain_community.document_loaders.csv_loader import CSVLoader
        loader = CSVLoader(file_path, encoding="utf-8")
        return loader.load()
    
    try:
        print(f"DEBUG: CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿é–‹å§‹: {file_path}")
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        df = pd.read_csv(file_path, encoding='utf-8')
        print(f"DEBUG: èª­ã¿è¾¼ã¿æˆåŠŸã€‚ç·è¡Œæ•°: {len(df)}")
        print(f"DEBUG: ã‚«ãƒ©ãƒ ä¸€è¦§: {list(df.columns)}")
        
        # éƒ¨ç½²åˆ—ã‚’æ­£ã—ãç‰¹å®šï¼ˆéƒ¨ç½²ã‚’æœ€å„ªå…ˆï¼‰
        dept_columns = ['éƒ¨ç½²', 'éƒ¨é–€', 'department', 'dept', 'å¾“æ¥­å“¡åŒºåˆ†']  # ã€Œéƒ¨ç½²ã€ã‚’æœ€å„ªå…ˆ
        dept_col = None
        
        print("DEBUG: éƒ¨ç½²åˆ—ã®æ¢ç´¢ï¼ˆä¿®æ­£ç‰ˆï¼‰:")
        for col in dept_columns:
            if col in df.columns:
                dept_col = col
                print(f"DEBUG: éƒ¨ç½²åˆ—ç™ºè¦‹: {col}")
                break
        
        if dept_col:
            print(f"DEBUG: éƒ¨ç½²åˆ— '{dept_col}' ã®å€¤ä¸€è¦§:")
            unique_depts = df[dept_col].unique()
            for dept in unique_depts:
                if pd.notna(dept):
                    count = len(df[df[dept_col] == dept])
                    print(f"  - {dept}: {count}äºº")
        
        # äººäº‹éƒ¨å¾“æ¥­å“¡ã‚’æŠ½å‡ºï¼ˆæ­£ã—ã„åˆ—ã§æ¤œç´¢ï¼‰
        hr_employees = None
        if dept_col:
            print("DEBUG: äººäº‹éƒ¨å¾“æ¥­å“¡ã®æŠ½å‡ºè©¦è¡Œï¼ˆä¿®æ­£ç‰ˆï¼‰:")
            print(f"DEBUG: æ¤œç´¢æ¡ä»¶: {dept_col}.str.contains('äººäº‹', na=False)")
            
            # äººäº‹éƒ¨ã‚’å«ã‚€éƒ¨ç½²ã‚’æ¤œç´¢
            hr_mask = df[dept_col].astype(str).str.contains('äººäº‹', na=False)
            hr_employees = df[hr_mask]
            print(f"DEBUG: äººäº‹éƒ¨å¾“æ¥­å“¡æ•°: {len(hr_employees)}")
            
            if len(hr_employees) > 0:
                print("DEBUG: äººäº‹éƒ¨å¾“æ¥­å“¡ç™ºè¦‹:")
                for idx, (_, emp) in enumerate(hr_employees.iterrows(), 1):
                    name = emp.get('æ°åï¼ˆãƒ•ãƒ«ãƒãƒ¼ãƒ ï¼‰', emp.get('æ°å', f'åå‰ä¸æ˜{idx}'))
                    dept = emp.get(dept_col, 'éƒ¨ç½²ä¸æ˜')
                    print(f"  {idx}. {name} - {dept}")
            else:
                print("DEBUG: 'äººäº‹'ã§è¦‹ã¤ã‹ã‚‰ãšã€‚åˆ¥ã®æ¤œç´¢èªã‚’è©¦è¡Œ:")
                # ã‚ˆã‚ŠæŸ”è»Ÿãªæ¤œç´¢
                search_terms = ['äººäº‹', 'HR', 'äººäº‹éƒ¨', 'äººäº‹èª²', 'ç·å‹™', 'ç®¡ç†']
                for search_term in search_terms:
                    mask = df[dept_col].astype(str).str.contains(search_term, na=False, case=False)
                    matches = df[mask]
                    print(f"  '{search_term}' æ¤œç´¢çµæœ: {len(matches)}äºº")
                    if len(matches) > 0:
                        print(f"    è©²å½“éƒ¨ç½²: {matches[dept_col].unique()}")
                        # æœ€åˆã«ãƒ’ãƒƒãƒˆã—ãŸæ¤œç´¢èªã‚’ä½¿ç”¨
                        if hr_employees is None or len(hr_employees) == 0:
                            hr_employees = matches
        
        # è¶…å¤§å‹çµ±åˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆ
        content_lines = []
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        content_lines.append("=" * 80)
        content_lines.append("ç¤¾å“¡åç°¿ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ - äººäº‹éƒ¨å¾“æ¥­å“¡å®Œå…¨ä¸€è¦§")
        content_lines.append("=" * 80)
        content_lines.append("")
        content_lines.append("ã€é‡è¦ã€‘äººäº‹éƒ¨ã«æ‰€å±ã—ã¦ã„ã‚‹å¾“æ¥­å“¡æƒ…å ±ã‚’ä¸€è¦§åŒ–")
        content_lines.append("äººäº‹éƒ¨ äººäº‹éƒ¨é–€ HRéƒ¨ äººäº‹èª² äººäº‹æ‹…å½“ äººäº‹éƒ¨å“¡ HRæ‹…å½“è€…")
        content_lines.append("äººäº‹ã‚¹ã‚¿ãƒƒãƒ• äººäº‹ãƒãƒ¼ãƒ  äººäº‹ãƒ¡ãƒ³ãƒãƒ¼ äººäº‹è·å“¡")
        content_lines.append("")
        
        # äººäº‹éƒ¨å¾“æ¥­å“¡ã®å®Œå…¨çµ±åˆæƒ…å ±
        if hr_employees is not None and not hr_employees.empty:
            content_lines.append(f"ğŸ¢ äººäº‹éƒ¨ç·å¾“æ¥­å“¡æ•°: {len(hr_employees)}äºº")
            content_lines.append("äººäº‹éƒ¨ã«æ‰€å±ã—ã¦ã„ã‚‹å¾“æ¥­å“¡ã®å®Œå…¨ãªä¸€è¦§ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š")
            content_lines.append("")
            content_lines.append("-" * 100)
            content_lines.append("äººäº‹éƒ¨å¾“æ¥­å“¡å®Œå…¨ãƒªã‚¹ãƒˆï¼ˆè©³ç´°ç‰ˆï¼‰")
            content_lines.append("-" * 100)
            content_lines.append("")
            
            # å„å¾“æ¥­å“¡ã®è©³ç´°æƒ…å ±ã‚’çµ±åˆ
            for idx, (_, emp) in enumerate(hr_employees.iterrows(), 1):
                content_lines.append(f"ğŸ”¸ äººäº‹éƒ¨å¾“æ¥­å“¡ {idx}ç•ªç›®")
                content_lines.append(f"   æ°å: {emp.get('æ°åï¼ˆãƒ•ãƒ«ãƒãƒ¼ãƒ ï¼‰', 'ä¸æ˜')}")
                content_lines.append(f"   ç¤¾å“¡ID: {emp.get('ç¤¾å“¡ID', 'ä¸æ˜')}")
                content_lines.append(f"   æ‰€å±: {emp.get(dept_col, 'äººäº‹éƒ¨')}")
                
                # å…¨é …ç›®ã®è©³ç´°æƒ…å ±
                for col, val in emp.items():
                    if pd.notna(val) and str(val).strip():
                        content_lines.append(f"   {col}: {val}")
                
                content_lines.append("")
                content_lines.append(f"   â€» {emp.get('æ°åï¼ˆãƒ•ãƒ«ãƒãƒ¼ãƒ ï¼‰', f'å¾“æ¥­å“¡{idx}')}ã¯äººäº‹éƒ¨ã«æ‰€å±ã—ã¦ã„ã‚‹å¾“æ¥­å“¡ã§ã™")
                content_lines.append("   " + "=" * 80)
                content_lines.append("")
            
            # ã‚µãƒãƒªãƒ¼ã‚»ã‚¯ã‚·ãƒ§ãƒ³
            content_lines.append("-" * 100)
            content_lines.append("ğŸ” äººäº‹éƒ¨å¾“æ¥­å“¡ã‚µãƒãƒªãƒ¼")
            content_lines.append("-" * 100)
            content_lines.append("")
            content_lines.append("ã€äººäº‹éƒ¨å¾“æ¥­å“¡åç°¿ã€‘")
            for idx, (_, emp) in enumerate(hr_employees.iterrows(), 1):
                name = emp.get('æ°åï¼ˆãƒ•ãƒ«ãƒãƒ¼ãƒ ï¼‰', f'å¾“æ¥­å“¡{idx}')
                emp_id = emp.get('ç¤¾å“¡ID', 'ä¸æ˜')
                role = emp.get('å½¹è·', 'è·å“¡')
                dept = emp.get(dept_col, 'äººäº‹éƒ¨')
                content_lines.append(f"{idx}. {name} (ç¤¾å“¡ID: {emp_id}) - å½¹è·: {role} - æ‰€å±: {dept}")
            
            content_lines.append("")
            content_lines.append("ã€é‡è¦ç¢ºèªã€‘")
            content_lines.append(f"ä¸Šè¨˜ãƒªã‚¹ãƒˆãŒäººäº‹éƒ¨ã«æ‰€å±ã—ã¦ã„ã‚‹å…¨{len(hr_employees)}äººã®å¾“æ¥­å“¡ã§ã™ã€‚")
            content_lines.append("äººäº‹éƒ¨å“¡ äººäº‹æ‹…å½“è€… HRéƒ¨å“¡ äººäº‹ã‚¹ã‚¿ãƒƒãƒ•ã®å®Œå…¨ãªä¸€è¦§æƒ…å ±ã€‚")
            
        else:
            content_lines.append("âš ï¸ äººäº‹éƒ¨å¾“æ¥­å“¡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            content_lines.append("ãƒ‡ãƒãƒƒã‚°æƒ…å ±:")
            content_lines.append(f"æ¤œç´¢å¯¾è±¡åˆ—: {dept_col}")
            if dept_col and dept_col in df.columns:
                content_lines.append("éƒ¨ç½²ä¸€è¦§:")
                for dept in df[dept_col].unique():
                    if pd.notna(dept):
                        content_lines.append(f"  - {dept}")
        
        # çµ±åˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ
        content = "\n".join(content_lines)
        
        # 1ã¤ã®å·¨å¤§ãªçµ±åˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ã—ã¦ä½œæˆ
        doc = Document(
            page_content=content,
            metadata={
                "source": file_path,
                "file_type": "csv",
                "document_type": "corrected_hr_database",
                "total_employees": len(df),
                "hr_employees": len(hr_employees) if hr_employees is not None else 0,
                "search_column": dept_col,
                "description": "ä¿®æ­£ç‰ˆäººäº‹éƒ¨å¾“æ¥­å“¡çµ±åˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹"
            }
        )
        
        return [doc]
    
    except Exception as e:
        print(f"DEBUG: CSVã‚¨ãƒ©ãƒ¼: {e}")
        # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å¾“æ¥ã®æ–¹æ³•ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        from langchain_community.document_loaders.csv_loader import CSVLoader
        loader = CSVLoader(file_path, encoding="utf-8")
        return loader.load()


def display_answer_with_sources(llm_response):
    """
    LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰å›ç­”ã¨ã‚½ãƒ¼ã‚¹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆãƒšãƒ¼ã‚¸æƒ…å ±ä»˜ãï¼‰ã‚’è¡¨ç¤º
    
    Args:
        llm_response: LLMã‹ã‚‰ã®å›ç­”ãƒ¬ã‚¹ãƒãƒ³ã‚¹
    """
    answer = llm_response["answer"]
    context_docs = llm_response.get("context", [])
    
    # å›ç­”ã¨ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’è¡¨ç¤º
    display_answer_with_page_info(answer, context_docs)